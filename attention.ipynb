{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_5S-TnnTbpVR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tìm Hiểu và Áp Dụng Cơ Chế Attention\n",
        "### Giới thiệu\n",
        "Trong phần này mình hướng dẫn các bạn cách cài đặt cơ chế attention trong bài toán seq2seq đơn giản hóa. \n",
        "\n",
        "Cơ chế attention chỉ đơn giản là trung bình có trọng số của những “thứ” mà chúng ta nghĩ nó cần thiết cho bài toán, điều đặc biệt là trọng số này do mô hình tự học được. Cụ thể, trong bài toán dịch máy ở ví dụ dưới, khi sử dụng cơ chế attention để phát sinh từ little, mình sẽ cần tính một vector context C là trung bình có trọng số của vector biểu diễn các từ mặt, trời, bé, nhỏ tương ứng với vector h1,h2,h3,h4, rồi sử dụng thêm vector context c này tại lúc dự đoán từ little, và nhớ rằng, trọng số này là các số scalar, được mô hình tự học \n",
        "\n",
        "![attention](https://github.com/pbcquoc/attention_tutorial/raw/master/img/attn_seq2seq.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "RkZkpr3qbpVT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import thư viện\n",
        "Mình sử dụng thêm cái lib của keras để hỗ trợ tiền xử lý cho nhanh, và tập trung chủ yếu vào phần cài đặt cơ chế attention\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "o43HsYMod3xO",
        "outputId": "be8b39bf-d2d2-44c9-facf-1daedf385340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from random import randint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "MQ9naryYbpVd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download tập huấn luyện\n",
        "Để minh họa cơ chế attention, mình sử dụng tập dataset tự phát sinh, với đầu vào là các câu biểu diễn ngày tháng năm của con người đọc, và nhãn là ngày tháng năm tương ứng do máy tính hiểu.\n",
        "\n",
        "Mình đã phát sinh tổng cộng 20k mẫu, trong đó 5k dùng để validation."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iDdQn4P6d3xl",
        "outputId": "ee6a4b1d-cac9-46bf-8e8f-53dea5d56b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "! curl --silent -L -o data.zip \"https://drive.google.com/uc?export=download&id=1d6eUqRstk7NIpyASzbuIsDvBdHEwfU0g\"\n",
        "! unzip -q data.zip\n",
        "! ls data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace data/machine_vocab.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "data.csv  human_vocab.json  machine_vocab.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d-PgIPRqbpVj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tiền xử lý\n",
        "Tập vocab mình xử dụng là các kí tự alphabet và số. Các bạn không cần phải filter, các kí tự đặt biệt trong tập dữ liệu"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "17yzXiMWd3xp",
        "outputId": "529006a9-221d-42a2-8fee-dc84d96f55b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    X = df[0].values\n",
        "    y = df[1].values\n",
        "    x_tok = Tokenizer(char_level=True, filters='')\n",
        "    x_tok.fit_on_texts(X)\n",
        "    y_tok = Tokenizer(char_level=True, filters='')\n",
        "    y_tok.fit_on_texts(y)\n",
        "    \n",
        "    X = x_tok.texts_to_sequences(X)\n",
        "    y = y_tok.texts_to_sequences(y)\n",
        "    \n",
        "    X = pad_sequences(X)\n",
        "    y = np.asarray(y)\n",
        "    \n",
        "    return X, y, x_tok.word_index, y_tok.word_index\n",
        "\n",
        "X, y, x_wid, y_wid= load_data('data/data.csv')\n",
        "x_id2w = dict(zip(x_wid.values(), x_wid.keys()))\n",
        "y_id2w = dict(zip(y_wid.values(), y_wid.keys()))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "print('train size: {} - test size: {}'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 18750 - test size: 6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yvTDTUDIbpVo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Định nghĩa các tham số"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Fy7StSyzd3xz",
        "outputId": "bcea276a-aada-4770-cb14-3387a7498dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# hidden size cho môt hình LSTM\n",
        "hidden_size = 128\n",
        "learning_rate = 0.001\n",
        "decoder_learning_ratio = 0.1\n",
        "\n",
        "# tập tự vựng của các câu đầu vào \n",
        "# +1 vì các bạn cần kí tự padding nhé!\n",
        "input_size = len(x_wid) + 1\n",
        "\n",
        "# +2 vì các bạn cần kí tự bắt đầu và kết thức\n",
        "output_size = len(y_wid) + 2\n",
        "# 2 kí tự này nằm ở cuối\n",
        "sos_idx = len(y_wid) \n",
        "eos_idx = len(y_wid) + 1\n",
        "\n",
        "max_length = y.shape[1]\n",
        "print(\"input vocab: {} - output vocab: {} - length of target: {}\".format(input_size, output_size, max_length))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input vocab: 35 - output vocab: 13 - length of target: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l6TVVm61bpVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Chuyển sang dạng chuỗi kí tự đọc được từ chuỗi số"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2LksDULqd3x3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoder_sentence(idxs, vocab):\n",
        "    text = ''.join([vocab[w] for w in idxs if (w > 0) and (w in vocab)])\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "almb2BMwbpVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Định nghĩa mô hình\n",
        "Ở phần này, các bạn cần định nghĩa 3 mô hình nhỏ\n",
        "* Encoder: là một mô hình LSTM, dùng để học biểu diễn của câu\n",
        "* Attention: dùng để học cách kết hợp để tạo ra context vector\n",
        "* Decoder: là một mô hình LSTM, chúng ta sẽ kết hợp context vector vào mô hình này để dự đoán các từ tại mỗi thời điểm\n",
        "\n",
        "![model](https://github.com/pbcquoc/pbcquoc.github.io/raw/master/images/attn_seq2seq_attn.png)\n",
        "\n",
        "### Encoder\n",
        "Mô hình này nhận đầu vào là các câu, các bạn có thể xem các hidden state h1,h2,h3,h4 như các biểu diễn của mỗi từ, và muốn tổng hợp context vector trên những thông tin này. \n",
        "### Attention\n",
        "Mô hình này học các trọng số alpha trên các h1,h2,h3,h4 rồi sau đó tổng hợp context vector theo trung bình có trọng số alpha này\n",
        "### Decoder\n",
        "Ở thời điểm dự đoán, các bạn sử dụng thêm context vector để bổ sung thông tin cho mô hình. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yVx1T2LJd3x6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # embedding vector của từ\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # mô hình GRU biến thể RNN để học vector biểu diễn của câu\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # input: SxB        \n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden # SxBxH, 1xBxH              \n",
        "\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attn ,self).__init__()\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        ### Mô hình nhận trạng thái hidden hiện tại của mô hình decoder, \n",
        "        ### và các hidden states của mô hình encoder\n",
        "        # encoder_outputs: TxBxH\n",
        "        # hidden: SxBxH\n",
        "        \n",
        "        # tranpose về đúng shape để nhận ma trận\n",
        "        encoder_outputs = torch.transpose(encoder_outputs, 0, 1) #BxTxH\n",
        "        hidden = torch.transpose(torch.transpose(hidden, 0, 1), 1, 2) # BxHxS\n",
        "        # tính e, chính là tương tác giữ hidden và các trạng thái ẩn của mô hình encoder \n",
        "        energies = torch.bmm(encoder_outputs, hidden) # BxTxS\n",
        "        energies = torch.transpose(energies, 1, 2) # BxSxT\n",
        "        # tính alpha, chính là trọng số của trung bình có trọng số cần tính bằng hàm softmax\n",
        "        attn_weights = F.softmax(energies, dim=-1) #BxSxT\n",
        "        \n",
        "        # tính context vector bằng trung binh có trọng số\n",
        "        output = torch.bmm(attn_weights, encoder_outputs) # BxSxH\n",
        "        \n",
        "        # trả về chiều cần thiết\n",
        "        output = torch.transpose(output, 0, 1) # SxBxH\n",
        "        attn_weights = torch.transpose(attn_weights, 0, 1) #SxBxT\n",
        "        \n",
        "        # return context vector và các trọng số alpha cho mục đích biểu diễn cơ chế attention\n",
        "        return output, attn_weights\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # vector biểu diễn cho các từ của output\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        # định nghĩa mô hình attention ở trên\n",
        "        self.attn = Attn(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # mô hình decoder là GRU\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        \n",
        "        # dự đoán các từ tại mội thời điểm, chúng ta nối 2 vector hidden và context lại với nhau \n",
        "        self.concat = nn.Linear(self.hidden_size*2, hidden_size)        \n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # input: SxB\n",
        "        # encoder_outputs: BxSxH\n",
        "        # hidden: 1xBxH\n",
        "        embedded = self.embedding(input) # 1xBxH\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        # biểu diễn của câu\n",
        "        rnn_output, hidden = self.gru(embedded, hidden)  #SxBxH, 1xBxH\n",
        "        # tính context vector dựa trên các hidden states\n",
        "        context, attn_weights = self.attn(rnn_output, encoder_outputs) # SxBxH\n",
        "        \n",
        "        # nối hidden state của mô hình decoder hiện tại và context vector để dự đoán \n",
        "        concat_input = torch.cat((rnn_output, context), -1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input)) #SxBxH\n",
        "        \n",
        "        # dự đoán kết quả tại mỗi thời điểm\n",
        "        output = self.out(concat_output) # SxBxoutput_size\n",
        "        return output, hidden, attn_weights\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRDEAU7WbpV3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Kiểm tra\n",
        "Chúng ta khởi tạo mô hình để kiểm tra xem mô hình có chạy được không, ít nhất là không bị lỗi về tính toán"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ArLtO8rsd3yA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = Encoder(input_size, hidden_size)\n",
        "decoder = Decoder(output_size, hidden_size, 0.1)\n",
        "\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "input_encoder = torch.randint(1, input_size, (34, 6), dtype=torch.long)\n",
        "encoder_outputs, hidden = encoder(input_encoder)\n",
        "input_decoder = torch.randint(1, output_size, (10, 6), dtype=torch.long)\n",
        "output, hidden, attn_weights = decoder(input_decoder, hidden, encoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "exEjFZUpbpV7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train/test\n",
        "Phần này chúng ta định nghĩa một số hàm để huấn luyện, dự đoán mô hình "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TnARQv5td3yG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_and_compute_loss(inputs, targets, encoder, decoder, criterion):\n",
        "    batch_size = inputs.size()[1]\n",
        "    \n",
        "    # định nghĩa 2 kí tự bắt đầu và kết thúc\n",
        "    sos = Variable(torch.ones((1, batch_size), dtype=torch.long)*sos_idx)\n",
        "    eos = Variable(torch.ones((1, batch_size), dtype=torch.long)*eos_idx)\n",
        "    \n",
        "    # input của mô hình decoder phải thêm kí tự bắt đầu\n",
        "    decoder_inputs = torch.cat((sos, targets), dim=0)\n",
        "    # output cần dự đoán của mô hình decoder phải thêm kí tự kết thúc\n",
        "    decoder_targets = torch.cat((targets, eos), dim=0)\n",
        "    \n",
        "    # forward tính hidden states của câu\n",
        "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
        "    # tính output của mô hình decoder\n",
        "    output, hidden, attn_weights = decoder(decoder_inputs, encoder_hidden, encoder_outputs)\n",
        "    \n",
        "    output = torch.transpose(torch.transpose(output, 0, 1), 1, 2) # BxCxS\n",
        "    decoder_targets = torch.transpose(decoder_targets, 0, 1)\n",
        "    # tính loss \n",
        "    loss = criterion(output, decoder_targets)\n",
        "    \n",
        "    return loss, output\n",
        "\n",
        "def train(inputs, targets,  encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    # khai báo train để mô hình biết là đang train hay test\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    # zero gradient, phải làm mỗi khi cập nhất gradient\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    \n",
        "    # tính loss dựa vào hàm đã định nghĩa ở trên\n",
        "    train_loss, output = forward_and_compute_loss(inputs, targets,encoder, decoder,criterion)    \n",
        "    \n",
        "    train_loss.backward()\n",
        "    # cập nhật một step\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "    # return loss để print :D\n",
        "    return train_loss.item()\n",
        "\n",
        "def evaluate(inputs, targets, encoder, decoder, criterion):\n",
        "    # báo cho mô hình biết đang test/eval\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    # tính loss\n",
        "    eval_loss, output = forward_and_compute_loss(inputs, targets, encoder, decoder,criterion)\n",
        "    output = torch.transpose(output, 1, 2)\n",
        "    # dự đoán của mỗi thời điểm các vị trí có prob lớn nhất\n",
        "    pred_idx = torch.argmax(output, dim=-1).squeeze(-1)\n",
        "    pred_idx = pred_idx.data.cpu().numpy()\n",
        "    \n",
        "    # return loss và kết quả dự đoán\n",
        "    return eval_loss.item(), pred_idx\n",
        "\n",
        "def predict(inputs, encoder, decoder, target_length=max_length):\n",
        "    ### Lúc dự đoán chúng ta cần tính kết quả ngay lập tức tại mỗi thời điểm, \n",
        "    ### rồi sau đó dừng từ được dự đoán để tính từ tiếp theo        \n",
        "    batch_size = inputs.size()[1]\n",
        "    \n",
        "    # input đầu tiên của mô hình decoder là kí tự bắt đầu, chúng ta dự đoán kí tự tiếp theo, sau đó lại dùng kí tự này để dự đoán từ kế tiếp\n",
        "    decoder_inputs = Variable(torch.ones((1, batch_size), dtype=torch.long)*sos_idx)\n",
        "    \n",
        "    # tính hidden state của mô hình encoder, cũng là vector biểu diễn của các từ, chúng ta cần tính context vector dựa trên những hidden states này\n",
        "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
        "    hidden = encoder_hidden\n",
        "    \n",
        "    preds = []\n",
        "    attn_weights = []\n",
        "    # chúng ta tính từng từ tại mỗi thời điểm\n",
        "    for i in range(target_length):\n",
        "        # dự đoán từ đầu tiên\n",
        "        output, hidden, attn_weight = decoder(decoder_inputs, hidden, encoder_outputs)\n",
        "        output = output.squeeze(dim=0)\n",
        "        pred_idx = torch.argmax(output, dim=-1)\n",
        "        \n",
        "        # thay đổi input tiếp theo bằng từ vừa được dự đoán\n",
        "        decoder_inputs = Variable(torch.ones((1, batch_size), dtype=torch.long)*pred_idx)\n",
        "        preds.append(decoder_inputs)\n",
        "        attn_weights.append(attn_weight.detach())\n",
        "    \n",
        "    preds = torch.cat(preds, dim=0)\n",
        "    preds = torch.transpose(preds, 0, 1)\n",
        "    attn_weights = torch.cat(attn_weights, dim=0)\n",
        "    attn_weights = torch.transpose(attn_weights, 0, 1)\n",
        "    return preds, attn_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VvrO070PcmWQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train và eval\n",
        "Trong phần này, chúng ta train mô hình, cũng như theo dõi độ lỗi, kết quả dự đoán tại mỗi epoch. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Fyfzas04d3yZ",
        "outputId": "174b5a9c-a703-46e8-fb17-7f795589d665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size)\n",
        "decoder = Decoder(output_size, hidden_size, 0.1)\n",
        "\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "X_val = torch.tensor(X_test, dtype=torch.long)\n",
        "y_val = torch.tensor(y_test, dtype=torch.long)\n",
        "X_val = torch.transpose(X_val, 0, 1)\n",
        "y_val = torch.transpose(y_val, 0, 1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for idx in range(len(X_train)//batch_size):\n",
        "        # input đầu vào của chúng ta là timestep first nhé. \n",
        "        X_train_batch = torch.tensor(X_train[batch_size*idx:batch_size*(idx+1)], dtype=torch.long)\n",
        "        y_train_batch = torch.tensor(y_train[batch_size*idx:batch_size*(idx+1)], dtype=torch.long)\n",
        "        \n",
        "        X_train_batch = torch.transpose(X_train_batch, 0, 1)\n",
        "        y_train_batch = torch.transpose(y_train_batch, 0, 1)\n",
        "        train_loss= train(X_train_batch, y_train_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "    eval_loss, preds = evaluate(X_val, y_val, encoder, decoder, criterion)\n",
        "    \n",
        "    print('Epoch {} - train loss: {:.3f} - eval loss: {:.3f}'.format(epoch, train_loss, eval_loss))\n",
        "    print_idx = np.random.randint(0, len(preds), 3)\n",
        "    for i in print_idx:\n",
        "        x_val = decoder_sentence(X_val[:,i].numpy(), x_id2w)\n",
        "        y_pred = decoder_sentence(preds[i], y_id2w)\n",
        "        print(\" {:<35s}\\t{:>10}\".format(x_val, y_pred))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - train loss: 0.533 - eval loss: 0.505\n",
            " 7 11 17                            \t7017-01-17\n",
            " tháng 3 14 1971                    \t1971-03-12\n",
            " 26 thg 3, 2013                     \t2013-03-16\n",
            "Epoch 1 - train loss: 0.197 - eval loss: 0.170\n",
            " ngày 03 tháng 04 năm 2008          \t2008-04-03\n",
            " 27 thg 11 1985                     \t1985-01-27\n",
            " 30 tháng 7 1975                    \t1975-07-30\n",
            "Epoch 2 - train loss: 0.112 - eval loss: 0.097\n",
            " 14, thg 7 2013                     \t2013-07-14\n",
            " 05/12/1990                         \t1990-02-05\n",
            " 27 thg 10, 1990                    \t1990-00-27\n",
            "Epoch 3 - train loss: 0.050 - eval loss: 0.044\n",
            " 10 thg 7, 1975                     \t1975-07-10\n",
            " 5 12 89                            \t1989-12-05\n",
            " thứ ba, ngày 31 tháng 5 năm 2016   \t2016-05-31\n",
            "Epoch 4 - train loss: 0.028 - eval loss: 0.025\n",
            " ngày 27 tháng 04 năm 2000          \t2000-04-27\n",
            " 27 thg 1, 1974                     \t1974-11-27\n",
            " 27 11 00                           \t2000-11-27\n",
            "Epoch 5 - train loss: 0.027 - eval loss: 0.021\n",
            " thứ tư, ngày 12 tháng 2 năm 1997   \t1997-02-12\n",
            " 11 tháng 1, 1981                   \t1981-11-11\n",
            " 4 tháng 12, 2012                   \t2012-12-04\n",
            "Epoch 6 - train loss: 0.018 - eval loss: 0.020\n",
            " 1 04 02                            \t2002-04-01\n",
            " 19 thg 11, 1984                    \t1984-11-19\n",
            " 21.10.12                           \t2012-10-21\n",
            "Epoch 7 - train loss: 0.022 - eval loss: 0.019\n",
            " 07.04.95                           \t1995-04-07\n",
            " 1 tháng 3 1970                     \t1970-03-01\n",
            " 09/03/1993                         \t1993-03-09\n",
            "Epoch 8 - train loss: 0.014 - eval loss: 0.018\n",
            " 05.07.80                           \t1980-07-05\n",
            " 9 thg 9, 1978                      \t1978-09-09\n",
            " ngày 29 tháng 12 năm 2004          \t2004-12-29\n",
            "Epoch 9 - train loss: 0.013 - eval loss: 0.017\n",
            " 06/08/1996                         \t1996-08-06\n",
            " 04, thg 3 1980                     \t1980-03-04\n",
            " 25 thg 3 1972                      \t1972-03-25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XL0477_kbpWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Predict\n",
        "Chúng ta dự đoán một vài mẫu và phân tích một số kết quả của cơ chế attention"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NglXGh77d3yc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds, attn_weights = predict(X_val ,encoder, decoder, target_length=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vRMILrf6d3yj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_attention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticks(np.arange(len(input_sentence)))\n",
        "    ax.set_xticklabels(list(input_sentence), rotation=90)\n",
        "    ax.set_yticks(np.arange(len(output_words)))\n",
        "    ax.set_yticklabels(list(output_words))\n",
        "    ax.grid()\n",
        "    ax.set_xlabel('Input Sequence')\n",
        "    ax.set_ylabel('Output Sequence')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FvmyeXyBbpWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Chọn ngẫu nhiên một câu trong tập validation để hiển thị. Khi hiển thị cơ chế attention, chúng ta có một cái nhìn về quá trình dự đoán của mô hình rõ ràng hơn, giúp đánh giá có thể interpretable hơn. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eFJvKjnOLL9W",
        "outputId": "d0f8a27b-85ba-46cb-d2e6-5289aad0ec58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "cell_type": "code",
      "source": [
        "show_idx = randint(0, len(preds))\n",
        "text_x = decoder_sentence(X_val[:,show_idx].numpy(), x_id2w)\n",
        "text_y = decoder_sentence(preds[show_idx].numpy(), y_id2w)\n",
        "attn_weight = attn_weights[show_idx, :, -len(text_x):]\n",
        "show_attention(text_x, text_y, attn_weight)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcgAAAE/CAYAAADCNlNLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAOAFBRIAAC8okAAAeEGBBADAi/8P5AZI3mz+VgYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}