{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "o43HsYMod3xO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f00c7861-2d9e-4ae3-e972-0ca3a3abf893"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from random import randint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "iDdQn4P6d3xl",
        "colab_type": "code",
        "outputId": "72baaeb2-1fbf-4c09-f1d8-7a1dd818db09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "! curl --silent -L -o data.zip \"https://drive.google.com/uc?export=download&id=1d6eUqRstk7NIpyASzbuIsDvBdHEwfU0g\"\n",
        "! unzip -q data.zip\n",
        "! ls data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace data/machine_vocab.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "data.csv  human_vocab.json  machine_vocab.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "17yzXiMWd3xp",
        "colab_type": "code",
        "outputId": "5f5b13cd-1ba8-432b-850f-7c1e02b6936e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    X = df[0].values\n",
        "    y = df[1].values\n",
        "    x_tok = Tokenizer(char_level=True, filters='')\n",
        "    x_tok.fit_on_texts(X)\n",
        "    y_tok = Tokenizer(char_level=True, filters='')\n",
        "    y_tok.fit_on_texts(y)\n",
        "    \n",
        "    X = x_tok.texts_to_sequences(X)\n",
        "    y = y_tok.texts_to_sequences(y)\n",
        "    \n",
        "    X = pad_sequences(X)\n",
        "    y = np.asarray(y)\n",
        "    \n",
        "    return X, y, x_tok.word_index, y_tok.word_index\n",
        "\n",
        "X, y, x_wid, y_wid= load_data('data/data.csv')\n",
        "x_id2w = dict(zip(x_wid.values(), x_wid.keys()))\n",
        "y_id2w = dict(zip(y_wid.values(), y_wid.keys()))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "print('train size: {} - test size: {}'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 18750 - test size: 6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fy7StSyzd3xz",
        "colab_type": "code",
        "outputId": "acafac39-5082-4949-f805-93fbc532fab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "learning_rate = 0.001\n",
        "decoder_learning_ratio = 0.1\n",
        "\n",
        "# plus 1 is padding token\n",
        "input_size = len(x_wid) + 1\n",
        "# plus 2 is sos and eos token\n",
        "output_size = len(y_wid) + 2\n",
        "sos_idx = len(y_wid) \n",
        "eos_idx = len(y_wid) + 1\n",
        "\n",
        "max_length = y.shape[1]\n",
        "print(\"input vocab: {} - output vocab: {} - length of target: {}\".format(input_size, output_size, max_length))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input vocab: 35 - output vocab: 13 - length of target: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2LksDULqd3x3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoder_sentence(idxs, vocab):\n",
        "    text = ''.join([vocab[w] for w in idxs if (w > 0) and (w in vocab)])\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yVx1T2LJd3x6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # input: SxB        \n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden # SxBxH, 1xBxH              \n",
        "\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attn ,self).__init__()\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # encoder_outputs: TxBxH\n",
        "        # hidden: SxBxH\n",
        "        encoder_outputs = torch.transpose(encoder_outputs, 0, 1) #BxTxH\n",
        "        hidden = torch.transpose(torch.transpose(hidden, 0, 1), 1, 2) # BxHxS\n",
        "        energies = torch.bmm(encoder_outputs, hidden) # BxTxS\n",
        "        energies = torch.transpose(energies, 1, 2) # BxSxT\n",
        "        attn_weights = F.softmax(energies, dim=-1) #BxSxT\n",
        "        \n",
        "        output = torch.bmm(attn_weights, encoder_outputs) # BxSxH\n",
        "        output = torch.transpose(output, 0, 1) # SxBxH\n",
        "        attn_weights = torch.transpose(attn_weights, 0, 1) #SxBxT\n",
        "        \n",
        "        return output, attn_weights\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attn = Attn(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.concat = nn.Linear(self.hidden_size*2, hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # input: SxB\n",
        "        # encoder_outputs: BxSxH\n",
        "        # hidden: 1xBxH\n",
        "        embedded = self.embedding(input) # 1xBxH\n",
        "        embedded = self.dropout(embedded)\n",
        "        rnn_output, hidden = self.gru(embedded, hidden)  #SxBxH, 1xBxH\n",
        "        context, attn_weights = self.attn(rnn_output, encoder_outputs) # SxBxH\n",
        "        concat_input = torch.cat((rnn_output, context), -1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input)) #SxBxH\n",
        "        \n",
        "        output = self.out(concat_output) # SxBxoutput_size\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ArLtO8rsd3yA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = Encoder(input_size, hidden_size)\n",
        "decoder = Decoder(output_size, hidden_size, 0.1)\n",
        "\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "input_encoder = torch.randint(1, input_size, (34, 6), dtype=torch.long)\n",
        "encoder_outputs, hidden = encoder(input_encoder)\n",
        "input_decoder = torch.randint(1, output_size, (10, 6), dtype=torch.long)\n",
        "output, hidden, attn_weights = decoder(input_decoder, hidden, encoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TnARQv5td3yG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_and_compute_loss(inputs, targets, encoder, decoder, criterion):\n",
        "    batch_size = inputs.size()[1]\n",
        "    \n",
        "    sos = Variable(torch.ones((1, batch_size), dtype=torch.long)*sos_idx)\n",
        "    eos = Variable(torch.ones((1, batch_size), dtype=torch.long)*eos_idx)\n",
        "    \n",
        "    decoder_inputs = torch.cat((sos, targets), dim=0)\n",
        "    decoder_targets = torch.cat((targets, eos), dim=0)\n",
        "    \n",
        "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
        "    output, hidden, attn_weights = decoder(decoder_inputs, encoder_hidden, encoder_outputs)\n",
        "    \n",
        "    output = torch.transpose(torch.transpose(output, 0, 1), 1, 2) # BxCxS\n",
        "    decoder_targets = torch.transpose(decoder_targets, 0, 1)\n",
        "    loss = criterion(output, decoder_targets)\n",
        "    \n",
        "    return loss, output\n",
        "\n",
        "def train(inputs, targets,  encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    \n",
        "    train_loss, output = forward_and_compute_loss(inputs, targets,encoder, decoder,criterion)    \n",
        "    \n",
        "    train_loss.backward()\n",
        "    \n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "            \n",
        "    return train_loss.item()\n",
        "\n",
        "def evaluate(inputs, targets, encoder, decoder, criterion):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    eval_loss, output = forward_and_compute_loss(inputs, targets, encoder, decoder,criterion)\n",
        "    output = torch.transpose(output, 1, 2)\n",
        "    pred_idx = torch.argmax(output, dim=-1).squeeze(-1)\n",
        "    pred_idx = pred_idx.data.cpu().numpy()\n",
        "    \n",
        "    return eval_loss.item(), pred_idx\n",
        "\n",
        "def predict(inputs, encoder, decoder, target_length=max_length):    \n",
        "    batch_size = inputs.size()[1]\n",
        "    decoder_inputs = Variable(torch.ones((1, batch_size), dtype=torch.long)*sos_idx)\n",
        "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
        "    hidden = encoder_hidden\n",
        "    preds = []\n",
        "    attn_weights = []\n",
        "    for i in range(target_length):\n",
        "        output, hidden, attn_weight = decoder(decoder_inputs, hidden, encoder_outputs)\n",
        "        output = output.squeeze(dim=0)\n",
        "        pred_idx = torch.argmax(output, dim=-1)\n",
        "        \n",
        "        decoder_inputs = Variable(torch.ones((1, batch_size), dtype=torch.long)*pred_idx)\n",
        "        preds.append(decoder_inputs)\n",
        "        attn_weights.append(attn_weight.detach())\n",
        "\n",
        "    preds = torch.cat(preds, dim=0)\n",
        "    preds = torch.transpose(preds, 0, 1)\n",
        "    attn_weights = torch.cat(attn_weights, dim=0)\n",
        "    attn_weights = torch.transpose(attn_weights, 0, 1)\n",
        "    return preds, attn_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l-h8o5Rtd3yW",
        "colab_type": "code",
        "outputId": "ff8f328e-8213-4708-df8d-6ef0dc3066fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "train(input_encoder, input_decoder, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.58532977104187"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "Fyfzas04d3yZ",
        "colab_type": "code",
        "outputId": "99efc6b3-04fb-41c5-8536-2be3c4629af2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size)\n",
        "decoder = Decoder(output_size, hidden_size, 0.1)\n",
        "\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "X_val = torch.tensor(X_test, dtype=torch.long)\n",
        "y_val = torch.tensor(y_test, dtype=torch.long)\n",
        "X_val = torch.transpose(X_val, 0, 1)\n",
        "y_val = torch.transpose(y_val, 0, 1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for idx in range(len(X_train)//batch_size):\n",
        "        X_train_batch = torch.tensor(X_train[batch_size*idx:batch_size*(idx+1)], dtype=torch.long)\n",
        "        y_train_batch = torch.tensor(y_train[batch_size*idx:batch_size*(idx+1)], dtype=torch.long)\n",
        "        \n",
        "        X_train_batch = torch.transpose(X_train_batch, 0, 1)\n",
        "        y_train_batch = torch.transpose(y_train_batch, 0, 1)\n",
        "        train_loss= train(X_train_batch, y_train_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "    eval_loss, preds = evaluate(X_val, y_val, encoder, decoder, criterion)\n",
        "    \n",
        "    print('Epoch {} - train loss: {:.3f} - eval loss: {:.3f}'.format(epoch, train_loss, eval_loss))\n",
        "    print_idx = np.random.randint(0, len(preds), 3)\n",
        "    for i in print_idx:\n",
        "        x_val = decoder_sentence(X_val[:,i].numpy(), x_id2w)\n",
        "        y_pred = decoder_sentence(preds[i], y_id2w)\n",
        "        print(\" {:<35s}\\t{:>10}\".format(x_val, y_pred))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - train loss: 0.772 - eval loss: 0.780\n",
            " 30 tháng 4 1989                    \t1989-07-00\n",
            " 16 thg 12, 1998                    \t1998-00-02\n",
            " thứ bảy, ngày 19 tháng 8 năm 2017  \t2018-07-22\n",
            "Epoch 1 - train loss: 0.113 - eval loss: 0.098\n",
            " 1 thg 12, 1991                     \t1991-12-11\n",
            " tháng 8 25 2017                    \t2017-08-25\n",
            " ngày 21 tháng 09 năm 1995          \t1995-09-21\n",
            "Epoch 2 - train loss: 0.059 - eval loss: 0.051\n",
            " 10.02.77                           \t1977-02-10\n",
            " 23 thg 4, 1980                     \t1980-04-23\n",
            " tháng 3 30 2000                    \t2000-03-30\n",
            "Epoch 3 - train loss: 0.051 - eval loss: 0.042\n",
            " thứ hai, ngày 12 tháng 6 năm 1972  \t1972-06-12\n",
            " 17 03 95                           \t1995-03-17\n",
            " tháng 4 3, 1989                    \t1989-04-33\n",
            "Epoch 4 - train loss: 0.046 - eval loss: 0.038\n",
            " 13 thg 5, 2008                     \t2008-05-13\n",
            " 05/08/2003                         \t2003-08-05\n",
            " 11 thg 11 2015                     \t2015-11-11\n",
            "Epoch 5 - train loss: 0.040 - eval loss: 0.036\n",
            " 24.09.03                           \t2003-09-24\n",
            " 17 tháng 4 1972                    \t1972-04-17\n",
            " 29, thg 6 1998                     \t1998-06-29\n",
            "Epoch 6 - train loss: 0.049 - eval loss: 0.036\n",
            " 4 thg 2, 2001                      \t2001-02-04\n",
            " tháng 4 3, 1996                    \t1996-04-03\n",
            " 30 thg 12 2018                     \t2018-12-30\n",
            "Epoch 7 - train loss: 0.038 - eval loss: 0.034\n",
            " 06.06.14                           \t2014-06-06\n",
            " 22 tháng 2, 2005                   \t2005-02-22\n",
            " 12/08/1990                         \t1990-08-12\n",
            "Epoch 8 - train loss: 0.033 - eval loss: 0.026\n",
            " tháng 12 12, 1974                  \t1974-12-12\n",
            " tháng 3 3 2011                     \t2011-03-33\n",
            " tháng 2 28 1997                    \t1997-02-28\n",
            "Epoch 9 - train loss: 0.023 - eval loss: 0.023\n",
            " 18 thg 2, 1991                     \t1991-02-18\n",
            " tháng 10 27 1970                   \t1970-10-27\n",
            " 4 tháng 12, 1977                   \t1977-12-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NglXGh77d3yc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds, attn_weights = predict(X_val ,encoder, decoder, target_length=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vRMILrf6d3yj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_attention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticks(np.arange(len(input_sentence)))\n",
        "    ax.set_xticklabels(list(input_sentence), rotation=90)\n",
        "    ax.set_yticks(np.arange(len(output_words)))\n",
        "    ax.set_yticklabels(list(output_words))\n",
        "    ax.grid()\n",
        "    ax.set_xlabel('Input Sequence')\n",
        "    ax.set_ylabel('Output Sequence')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eFJvKjnOLL9W",
        "colab_type": "code",
        "outputId": "524bb609-521f-4648-bbfe-9922439ca751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "show_idx = randint(0, len(preds))\n",
        "text_x = decoder_sentence(X_val[:,show_idx].numpy(), x_id2w)\n",
        "text_y = decoder_sentence(preds[show_idx].numpy(), y_id2w)\n",
        "attn_weight = attn_weights[show_idx, :, -len(text_x):]\n",
        "show_attention(text_x, text_y, attn_weight)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAFUCAYAAAAzods1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}